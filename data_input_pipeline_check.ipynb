{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking TF Dataset Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "# from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "\n",
    "from collections import namedtuple\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_labelId = \"cityscapes/gtFine/train/bochum/bochum_000000_027057_gtFine_labelIds.png\"\n",
    "path_trainId = \"cityscapes/gtFine/train/bochum/bochum_000000_027057_gtFine_labelTrainIds.png\"\n",
    "path_color =   \"cityscapes/gtFine/train/bochum/bochum_000000_027057_gtFine_color.png\"\n",
    "\n",
    "img_labelId = cv2.imread(path_labelId,0)\n",
    "img_trainId = cv2.imread(path_trainId,0)\n",
    "img_color = cv2.imread(path_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_labelId.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_labelId[:,1200].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_trainId[:,1200].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8), dpi=150)\n",
    "plt.imshow(img_color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes_total = 34\n",
    "n_train_classes = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "def get_labels():\n",
    "\n",
    "    # a label and all meta information\n",
    "    Label = namedtuple( 'Label' , [\n",
    "\n",
    "        'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                        # We use them to uniquely name a class\n",
    "\n",
    "        'id'          , # An integer ID that is associated with this label.\n",
    "                        # The IDs are used to represent the label in ground truth images\n",
    "                        # An ID of -1 means that this label does not have an ID and thus\n",
    "                        # is ignored when creating ground truth images (e.g. license plate).\n",
    "                        # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                        # evaluation server.\n",
    "\n",
    "        'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                        # ground truth images with train IDs, using the tools provided in the\n",
    "                        # 'preparation' folder. However, make sure to validate or submit results\n",
    "                        # to our evaluation server using the regular IDs above!\n",
    "                        # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                        # are mapped to the same class in the ground truth images. For the inverse\n",
    "                        # mapping, we use the label that is defined first in the list below.\n",
    "                        # For example, mapping all void-type classes to the same ID in training,\n",
    "                        # might make sense for some approaches.\n",
    "                        # Max value is 255!\n",
    "\n",
    "        'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "        'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                        # on category level.\n",
    "\n",
    "        'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "        'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                        # during evaluations or not\n",
    "\n",
    "        'color'       , # The color of this label\n",
    "        ] )\n",
    "\n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # A list of all labels\n",
    "    #--------------------------------------------------------------------------------\n",
    "\n",
    "    # Please adapt the train IDs as appropriate for your approach.\n",
    "    # Note that you might want to ignore labels with ID 255 during training.\n",
    "    # Further note that the current train IDs are only a suggestion. You can use whatever you like.\n",
    "    # Make sure to provide your results using the original IDs and not the training IDs.\n",
    "    # Note that many IDs are ignored in evaluation and thus you never need to predict these!\n",
    "    \n",
    "\n",
    "    labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,        0 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,        0 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,        0 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,        0 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,        0 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,        0 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,        0 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        1 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        2 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,        0 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,        0 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        3 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        4 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        5 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,        0 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,        0 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,        0 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        6 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,        0 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        7 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        8 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        9 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,       10 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       11 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       12 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       13 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       15 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,        0 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,        0 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       17 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       18 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       19 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "    ]\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = get_labels() # a list of named tuples\n",
    "id2label = { label.id : label for label in labels }\n",
    "catid2label = { label.categoryId : label for label in labels }\n",
    "trainId2label = { label.trainId : label for label in labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_rgb(mask):\n",
    "    mask_rgb = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    for i in range(0,n_train_classes):\n",
    "        idx = mask[:,:,0]==i\n",
    "        mask_rgb[idx] = trainId2label[i].color\n",
    "        # mask_rgb[idx] = catid2label[i].color\n",
    "        # mask_rgb[idx] = id2label[i].color\n",
    "    return mask_rgb\n",
    "\n",
    "\n",
    "def display(display_list):\n",
    "    plt.figure(figsize=(15, 5), dpi=200)\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        #plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(raw_record):\n",
    "    keys_to_features = {\n",
    "      'image/encoded': tf.io.FixedLenFeature((), tf.string),\n",
    "      'image/format': tf.io.FixedLenFeature((), tf.string),\n",
    "      'image/height': tf.io.FixedLenFeature((), tf.int64),\n",
    "      'image/width': tf.io.FixedLenFeature((), tf.int64),\n",
    "      'image/channels': tf.io.FixedLenFeature((), tf.int64),\n",
    "      'label/encoded': tf.io.FixedLenFeature((), tf.string),\n",
    "      'label/format': tf.io.FixedLenFeature((), tf.string),\n",
    "    }\n",
    "\n",
    "    parsed = tf.io.parse_single_example(raw_record, keys_to_features)\n",
    "\n",
    "    image = tf.image.decode_image(tf.reshape(parsed['image/encoded'], shape=[]), 3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image.set_shape([None, None, 3])\n",
    "\n",
    "    label = tf.image.decode_image(tf.reshape(parsed['label/encoded'], shape=[]), 1)\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    label.set_shape([None, None, 1])\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def read_tfrecord(serialized_example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature((), tf.string),\n",
    "        'segmentation': tf.io.FixedLenFeature((), tf.string),\n",
    "        'height': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'width': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'image_depth': tf.io.FixedLenFeature((), tf.int64),\n",
    "        'mask_depth': tf.io.FixedLenFeature((), tf.int64),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(serialized_example, feature_description)\n",
    "    \n",
    "\n",
    "    #image = tf.io.parse_tensor(example['image'], out_type = tf.float32)\n",
    "    image = tf.io.parse_tensor(example['image'], out_type = tf.uint8)\n",
    "    image_shape = [example['height'], example['width'], 3]\n",
    "    image = tf.reshape(image, image_shape)\n",
    "    \n",
    "    mask = tf.io.parse_tensor(example['segmentation'], out_type = tf.uint8)\n",
    "    mask_shape = [example['height'], example['width'], 1]\n",
    "    mask = tf.reshape(mask, mask_shape)\n",
    "    \n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def get_dataset_from_tfrecord(tfrecord_dir):\n",
    "    tfrecord_dataset = tf.data.TFRecordDataset(tfrecord_dir)\n",
    "    # parsed_dataset = tfrecord_dataset.map(read_tfrecord)\n",
    "    parsed_dataset = tfrecord_dataset.map(parse_record)\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check dataset and input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 512\n",
    "img_width = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_crop(input_image, input_mask):\n",
    "    stacked_image = tf.concat([input_image, input_mask], axis=2)\n",
    "    cropped_image = tf.image.random_crop(stacked_image, size=[img_height, img_width, 4])\n",
    "    return cropped_image[:,:,0:3], cropped_image[:,:,-1]\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def mask_to_categorical(image, mask):\n",
    "    mask = tf.squeeze(mask)\n",
    "    mask = tf.one_hot(tf.cast(mask, tf.int32), n_train_classes)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image_train(input_image, input_mask):\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.uint8)\n",
    "    input_mask = tf.cast(input_mask, tf.uint8)\n",
    "    \n",
    "    input_image = tf.image.resize(input_image, (768, 1536))\n",
    "    input_mask = tf.image.resize(input_mask, (768, 1536))\n",
    "    \n",
    "    #if tf.random.uniform(()) > 0.5:\n",
    "    #    input_image = tf.image.flip_left_right(input_image)\n",
    "    #    input_mask = tf.image.flip_left_right(input_mask)\n",
    "    \n",
    "    input_image = tf.squeeze(input_image)    \n",
    "    input_image, input_mask = random_crop(input_image, input_mask)\n",
    "        \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0    \n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image_test(input_image, input_mask):\n",
    "    input_image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    input_mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfrecord_dir = \"records/trainIds_train.record\"\n",
    "valid_tfrecord_dir = \"records/trainIds_val.record\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "valid_dataset = get_dataset_from_tfrecord(valid_tfrecord_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, mask) in enumerate(train_dataset.take(5)):\n",
    "    sample_image, sample_mask = image.numpy(), mask.numpy()\n",
    "    \n",
    "sample_mask = label_to_rgb(sample_mask)\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = train_dataset.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "valid = valid_dataset.map(load_image_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, mask) in enumerate(train.take(5)):\n",
    "    preprocessed_image, preprocessed_mask = image, mask\n",
    "    \n",
    "print(preprocessed_mask.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_mask = tf.argmax(preprocessed_mask, axis=-1)\n",
    "preprocessed_mask = preprocessed_mask[..., tf.newaxis]\n",
    "preprocessed_mask = label_to_rgb(preprocessed_mask.numpy())\n",
    "display([preprocessed_image, preprocessed_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
