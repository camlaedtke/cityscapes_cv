{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LambdaCallback\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from utils.plot_utils import plot_iou_trainId, plot_iou_catId\n",
    "from utils.data_utils import get_labels, parse_record, get_dataset_from_tfrecord\n",
    "from models.hrnet_keras import HRNet\n",
    "from models.u2net import U2NET\n",
    "\n",
    "K.clear_session()\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "def enable_amp():\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "# print(physical_devices,\"\\n\")\n",
    "# enable_amp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "# sudo apt-get install apt-transport-https ca-certificates gnupg\n",
    "# curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n",
    "# sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "# gcloud init --console-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_train.record /home/ubuntu/cityscapes_cv/records/\n",
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_val.record /home/ubuntu/cityscapes_cv/records/\n",
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_train_extra.record /home/ubuntu/cityscapes_cv/records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine = True\n",
    "\n",
    "if fine:\n",
    "    train_tfrecord_dir = \"records/trainIds_train.record\"\n",
    "    test_tfrecord_dir = \"records/trainIds_val.record\"\n",
    "else:\n",
    "    train_tfrecord_dir = \"records/trainIds_train_extra.record\"\n",
    "\n",
    "img_height = 64 # 512\n",
    "img_width = 128 # 1024\n",
    "n_classes = 20\n",
    "\n",
    "labels = get_labels()\n",
    "trainid2label = { label.trainId : label for label in labels }\n",
    "catId2label = { label.categoryId : label for label in labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def random_crop(image, mask):\n",
    "    \"\"\"\n",
    "    Inputs: full resolution image and mask\n",
    "    A scale between 0.5 and 1.0 is randomly chosen. \n",
    "    Then, we multiply original height and width by the scale, \n",
    "    and randomly crop to the scaled height and width.\n",
    "    \"\"\"\n",
    "    \n",
    "    scales = tf.convert_to_tensor(np.array(\n",
    "        [0.25, 0.3125, 0.375, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0]))\n",
    "    scale = scales[tf.random.uniform(shape=[], minval=0, maxval=13, dtype=tf.int32)]\n",
    "    scale = tf.cast(scale, tf.float32)\n",
    "    \n",
    "    shape = tf.cast(tf.shape(image), tf.float32)\n",
    "    h = tf.cast(shape[0] * scale, tf.int32)\n",
    "    w = tf.cast(shape[1] * scale, tf.int32)\n",
    "    combined_tensor = tf.concat([image, mask], axis=2)\n",
    "    combined_tensor = tf.image.random_crop(combined_tensor, size=[h, w, 4])\n",
    "    return combined_tensor[:,:,0:3], combined_tensor[:,:,-1]\n",
    "\n",
    "@tf.function\n",
    "def mask_to_categorical(image, mask):\n",
    "    mask = tf.squeeze(mask)\n",
    "    mask = tf.one_hot(tf.cast(mask, tf.int32), n_classes)\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def load_image_train(input_image, input_mask):\n",
    "    \n",
    "    image = tf.cast(input_image, tf.uint8)\n",
    "    mask = tf.cast(input_mask, tf.uint8)\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "         \n",
    "    # if tf.random.uniform(()) > 0.5:\n",
    "        # image, mask = random_crop(image, mask)\n",
    "        # mask = tf.expand_dims(mask, axis=-1)\n",
    "    \n",
    "    image = tf.image.resize(image, (img_height, img_width))\n",
    "    mask = tf.image.resize(mask, (img_height, img_width))\n",
    "    \n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        image = tf.image.random_brightness(image, 0.05)\n",
    "        image = tf.image.random_saturation(image, 0.6, 1.6)\n",
    "        image = tf.image.random_contrast(image, 0.7, 1.3)\n",
    "        image = tf.image.random_hue(image, 0.05)\n",
    "    \n",
    "    image, mask = mask_to_categorical(image, mask)\n",
    "    mask = tf.squeeze(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def load_image_test(input_image, input_mask):\n",
    "    image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "    \n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image, mask = mask_to_categorical(image, mask)\n",
    "    mask = tf.squeeze(mask)\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def load_image_eval(input_image, input_mask):\n",
    "    input_image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    # input_mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "    return input_image, input_mask\n",
    "\n",
    "\n",
    "def load_image_train_no_aug(input_image, input_mask):\n",
    "    input_image = tf.image.resize(input_image, (img_height, img_width))\n",
    "    input_mask = tf.image.resize(input_mask, (img_height, img_width))\n",
    "    \n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    input_image, input_mask = mask_to_categorical(input_image, input_mask)\n",
    "    input_mask = tf.squeeze(input_mask)\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine:\n",
    "    TRAIN_LENGTH = 2975\n",
    "    TEST_LENGTH = 500\n",
    "else:\n",
    "    TRAIN_LENGTH = 18000\n",
    "    TEST_LENGTH = 1998\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "ACCUM_STEPS = 6\n",
    "BUFFER_SIZE = 500\n",
    "ADJ_BATCH_SIZE = BATCH_SIZE // ACCUM_STEPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine:\n",
    "    train_ds = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "    test_ds = get_dataset_from_tfrecord(test_tfrecord_dir)\n",
    "else:\n",
    "    all_ds = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "    train_ds = all_ds.skip(TEST_LENGTH)\n",
    "    test_ds = all_ds.take(TEST_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: random crop the images and masks, flip them\n",
    "train = train_ds.map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = test_ds.map(load_image_test)\n",
    "eval = test_ds.map(load_image_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(ADJ_BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test.batch(ADJ_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_rgb(mask):\n",
    "    h = mask.shape[0]\n",
    "    w = mask.shape[1]\n",
    "    mask_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for val, key in trainid2label.items():\n",
    "        indices = mask == val\n",
    "        mask_rgb[indices.squeeze()] = key.color \n",
    "    return mask_rgb\n",
    "\n",
    "\n",
    "def display(display_list, title=True):\n",
    "    plt.figure(figsize=(15, 5)) # dpi=200\n",
    "    if title:\n",
    "        title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        if title:\n",
    "            plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in train.take(21): # 16\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "sample_mask = tf.argmax(sample_mask, axis=-1)\n",
    "sample_mask = sample_mask[..., tf.newaxis]\n",
    "sample_mask = label_to_rgb(sample_mask.numpy())\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if fine:\n",
    "    MODEL_PATH = \"weights/\"+model.name+\".h5\"\n",
    "else:\n",
    "    MODEL_PATH = \"weights/\"+model.name+\"_coarse.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.squeeze(pred_mask)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    pred_mask = label_to_rgb(pred_mask.numpy())\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def show_predictions():\n",
    "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
    "    if \"u2net\" in model.name:\n",
    "        pred_mask = pred_mask[0]\n",
    "    display([sample_image, sample_mask, create_mask(pred_mask)])\n",
    "\n",
    "        \n",
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_file = \"cityscapes_fine_hrnet.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def iou_macro_coef(y_true, y_pred):\n",
    "    smooth = 1\n",
    "    iou_total = 0\n",
    "    for i in range(1, n_classes):\n",
    "        tp = tf.math.reduce_sum(y_pred[:,:,:,i] * y_true[:,:,:,i], axis=(1,2))\n",
    "        fn = tf.math.reduce_sum(y_true[:,:,:,i] * (1 - y_pred[:,:,:,i]), axis=(1,2)) \n",
    "        fp = tf.math.reduce_sum(y_pred[:,:,:,i] * (1 - y_true[:,:,:,i]), axis=(1,2)) \n",
    "        iou = tf.math.reduce_mean(tf.math.divide_no_nan(tp+smooth, tp+fn+fp+smooth), axis=0)\n",
    "        iou_total += iou\n",
    "\n",
    "    iou_macro = iou_total / (n_classes - 1)\n",
    "    return iou_macro\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    def train_fn(opt, x, y):\n",
    "        with tf.GradientTape() as tape :\n",
    "            output = model(x, training=True)\n",
    "            loss = cce_loss(y, output)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))\n",
    "        return loss\n",
    "\n",
    "    per_example_losses = strategy.run(train_fn, args=(opt,x, y,))\n",
    "    mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, per_example_losses, axis=None)\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y) :\n",
    "    def test_fn(x, y) :\n",
    "        output = model(x, training=False)\n",
    "        loss = cce_loss(y, output)\n",
    "        miou = iou_macro_coef(y, output)\n",
    "        return loss, miou\n",
    "\n",
    "    pe_loss, pe_miou = strategy.run(test_fn, args=(x, y,))\n",
    "    mean_loss = strategy.reduce(tf.distribute.ReduceOp.MEAN, pe_loss, axis=None)\n",
    "    mean_miou = strategy.reduce(tf.distribute.ReduceOp.MEAN, pe_miou, axis=None)\n",
    "    return mean_loss, mean_miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    \n",
    "     logfile = Path(logger_file)\n",
    "\n",
    "    if not logfile.exists() :\n",
    "        tmpf = logfile.open(\"w+\")\n",
    "        top_text = \"epoch, loss, val_loss, miou\\n\"\n",
    "        tmpf.write(top_text)\n",
    "        tmpf.close()\n",
    "    \n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        # model = U2NET(input_height=img_height, input_width=img_width, n_classes=n_classes)\n",
    "        model = HRNet(input_height=img_height, input_width=img_width, n_classes=20, W=40)\n",
    "\n",
    "        #  model.load_weights(\"weights/\"+model.name+\"_coarse.h5\")\n",
    "\n",
    "        cce_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        initial_lr = 0.01 \n",
    "        end_lr = 1e-5 \n",
    "        decay_steps = 120000 \n",
    "        learning_rate_fn = PolynomialDecay(\n",
    "            initial_lr,\n",
    "            decay_steps,\n",
    "            end_lr,\n",
    "            power=0.9\n",
    "        )\n",
    "\n",
    "        optimizer = SGD(learning_rate=learning_rate_fn, momentum=0.9, decay=0.0005)\n",
    "\n",
    "        train_loss_metric = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "        valid_loss_metric = tf.keras.metrics.Mean('val_loss', dtype=tf.float32)\n",
    "\n",
    "        valid_miou_metric = tf.keras.metrics.Mean('valid_iou_coef', dtype=tf.float32)\n",
    "\n",
    "        history = {\"loss\": [], \"val_loss\": [], \"val_iou_coef\": []}\n",
    "\n",
    "        start_time = time()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time()\n",
    "\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                loss = train_step(x_batch_train, y_batch_train)\n",
    "                # Update training metrics.\n",
    "                train_loss_metric.update_state(loss)\n",
    "                print(\"\\r Batch {} -- loss: {:.4f}\".format(step, loss.numpy()), end='')\n",
    "\n",
    "\n",
    "            for x_batch_val, y_batch_val in test_dataset:\n",
    "                loss, miou = test_step(x_batch_val, y_batch_val)\n",
    "                valid_loss_metric.update_state(loss)\n",
    "                valid_miou_metric.update_state(miou)\n",
    "\n",
    "            train_loss = train_loss_metric.result().numpy() \n",
    "            valid_loss = valid_loss_metric.result().numpy()\n",
    "            valid_miou = valid_miou_metric.result().numpy()\n",
    "\n",
    "            history['loss'].append(train_loss)\n",
    "            history['val_loss'].append(valid_loss)\n",
    "            history['val_iou_coef'].append(valid_miou)\n",
    "            \n",
    "            tmpf = logfile.open(\"a+\")\n",
    "            tmpf.write(\",\".join([str(epoch), str(train_loss), str(valid_loss), str(valid_miou)]) + \"\\n\")\n",
    "            tmpf.close()\n",
    "\n",
    "            train_loss_metric.reset_states()     \n",
    "            valid_loss_metric.reset_states()\n",
    "            valid_miou_metric.reset_states()\n",
    "            \n",
    "            \n",
    "\n",
    "            t_epoch = time() - epoch_start_time\n",
    "            template = '\\n Epoch {} -- Time: {:.2f}s, Loss: {:.4f}, Val Loss: {:.4f}, Val mIoU: {:.4f}'\n",
    "            print(template.format(epoch+1, t_epoch, train_loss, valid_loss, valid_miou))\n",
    "            \n",
    "            model.save(MODEL_PATH)\n",
    "\n",
    "            # show_predictions()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=20):\n",
    "    start_time = time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time()\n",
    "        \n",
    "        total_iou = 0\n",
    "        total_loss = 0\n",
    "        # get trainable variables\n",
    "        train_vars = model.trainable_variables \n",
    "        # Create empty gradient list (not a tf.Variable list)\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in train_vars]\n",
    "\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "            loss_value, iou_value, accum_gradient = accumilate_train_step(\n",
    "                x_batch_train, y_batch_train, accum_gradient, train_vars)\n",
    "            \n",
    "            total_loss += loss_value\n",
    "            total_iou += iou_value\n",
    "\n",
    "            # Now, after executing all the tapes you needed, we apply the optimization step\n",
    "            if (step > 0) and (step % ACCUM_STEPS == 0):\n",
    "                \n",
    "                optimizer.apply_gradients(zip(accum_gradient, train_vars))\n",
    "\n",
    "                loss = total_loss / ACCUM_STEPS\n",
    "                iou = total_iou / ACCUM_STEPS\n",
    "\n",
    "                # Update training metrics.\n",
    "                train_loss_metric.update_state(total_loss)\n",
    "                train_miou_metric.update_state(iou)\n",
    "\n",
    "                print(\"\\r Batch {} -- loss: {:.4f}, IoU: {:.4f}\".format(\n",
    "                    (step // ACCUM_STEPS), loss.numpy(), iou.numpy()), end='')\n",
    "\n",
    "                total_loss = 0\n",
    "                total_iou = 0\n",
    "                # get trainable variables\n",
    "                train_vars = model.trainable_variables \n",
    "                # Create empty gradient list (not a tf.Variable list)\n",
    "                accum_gradient = [tf.zeros_like(this_var) for this_var in train_vars]\n",
    "\n",
    "                \n",
    "        for x_batch_val, y_batch_val in test_dataset:\n",
    "            test_step(x_batch_val, y_batch_val)\n",
    "        \n",
    "\n",
    "        train_loss = train_loss_metric.result().numpy() / ACCUM_STEPS\n",
    "        train_miou = train_miou_metric.result().numpy()\n",
    "\n",
    "        valid_loss = valid_loss_metric.result().numpy()\n",
    "        valid_miou = valid_miou_metric.result().numpy()\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(valid_loss)\n",
    "        history['iou_coef'].append(train_miou)\n",
    "        history['val_iou_coef'].append(valid_miou)\n",
    "\n",
    "        train_loss_metric.reset_states()\n",
    "        train_miou_metric.reset_states()        \n",
    "\n",
    "        valid_loss_metric.reset_states()\n",
    "        valid_miou_metric.reset_states()\n",
    "\n",
    "        t_epoch = time() - epoch_start_time\n",
    "        template = '\\n Epoch {} -- Time: {:.2f}s, Loss: {:.4f} , mIoU: {:.4f}, Val Loss: {:.4f}, Val mIoU: {:.4f}'\n",
    "        print(template.format(epoch+1, t_epoch, train_loss, train_miou, valid_loss, valid_miou))\n",
    "        \n",
    "        model.save(MODEL_PATH)\n",
    "\n",
    "        #show_predictions()\n",
    "        \n",
    "    end_time = time()\n",
    "    t_minutes = (end_time - start_time) // 60\n",
    "    print(\"Training finished in {:.2f} minutes\".format(t_minutes))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, history = train(epochs=20)\n",
    "\n",
    "# No accumilate: 411s per epoch\n",
    "# Accumilate: 581s per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    start_time = time()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time()\n",
    "\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "            loss, iou = train_step(optimizer, x_batch_train, y_batch_train)\n",
    "            print(\"\\r Batch {} -- loss: {:.4f}, IoU: {:.4f}\".format(step, loss.numpy(), iou.numpy()), end='')\n",
    "\n",
    "        for x_batch_val, y_batch_val in test_dataset:\n",
    "            test_step(x_batch_val, y_batch_val)\n",
    "\n",
    "        train_loss = train_loss_metric.result().numpy()\n",
    "        train_miou = train_miou_metric.result().numpy()\n",
    "\n",
    "        valid_loss = valid_loss_metric.result().numpy()\n",
    "        valid_miou = valid_miou_metric.result().numpy()\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(valid_loss)\n",
    "        history['iou_coef'].append(train_miou)\n",
    "        history['val_iou_coef'].append(valid_miou)\n",
    "\n",
    "        train_loss_metric.reset_states()\n",
    "        train_miou_metric.reset_states()        \n",
    "\n",
    "        valid_loss_metric.reset_states()\n",
    "        valid_miou_metric.reset_states()\n",
    "\n",
    "        t_epoch = time() - epoch_start_time\n",
    "        template = '\\n Epoch {} -- Time: {:.2f}s, Loss: {:.4f} , mIoU: {:.4f}, Val Loss: {:.4f}, Val mIoU: {:.4f}'\n",
    "        print (template.format(epoch+1, t_epoch, train_loss, train_miou, valid_loss, valid_miou))\n",
    "\n",
    "        show_predictions()\n",
    "\n",
    "    end_time = time()\n",
    "    t_minutes = (end_time - start_time) // 60\n",
    "    print(\"Training finished in {:.2f} minutes\".format(t_minutes))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model):\n",
    "        \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(1,2,1)  \n",
    "    if \"u2net\" in model.name:\n",
    "        plt.plot(history['d0_loss'], 'r', label='Training loss')\n",
    "        plt.plot(history['val_d0_loss'], 'b', label='Validation loss')\n",
    "    else: \n",
    "        plt.plot(history['loss'], 'r', label='Training loss')\n",
    "        plt.plot(history['val_loss'], 'b', label='Validation loss')\n",
    "    plt.title(\"Loss: \"+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    if \"u2net\" in model.name:\n",
    "        plt.plot(history['d0_iou_coef'], 'r', label='IoU coefficient')\n",
    "        plt.plot(history['val_d0_iou_coef'], 'b', label='Validation IoU coefficient')\n",
    "    else:\n",
    "        plt.plot(history['iou_coef'], 'r', label='IoU coefficient')\n",
    "        plt.plot(history['val_iou_coef'], 'b', label='Validation IoU coefficient')\n",
    "    plt.title('IoU Coefficient: '+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    if fine:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves.png\")\n",
    "    else:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves_coarse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_iou(model, dataset, n_samples):\n",
    "    \n",
    "    iou_macro_scores = np.zeros((n_samples,))\n",
    "    inf_times = np.zeros((n_samples, ))\n",
    "    miou_op =  tf.keras.metrics.MeanIoU(num_classes=n_classes-1)\n",
    "    \n",
    "    for idx, (image, mask) in enumerate(dataset):\n",
    "        print(\"\\r Predicting {} \\ {} \".format(idx+1, n_samples), end='')\n",
    "        \n",
    "        X = np.expand_dims(image.numpy(), axis=0)\n",
    "        y_true = np.expand_dims(mask.numpy(), axis=0)\n",
    "        \n",
    "        t_start = time()\n",
    "        y_pred = model.predict(X)\n",
    "        t_end = time()\n",
    "        t_inf = t_end-t_start\n",
    "        \n",
    "        inf_times[idx] = t_inf\n",
    "        \n",
    "        if \"u2net\" in model.name:\n",
    "            y_pred = y_pred[0]\n",
    "            \n",
    "        # y_pred = tf.image.resize(y_pred, (1024, 2048))\n",
    "        threshold = tf.math.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
    "        \n",
    "        y_pred = tf.cast(tf.squeeze(y_pred, axis=0), tf.int32)\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=0), tf.int32)\n",
    "        \n",
    "        y_true = tf.argmax(y_true[:,:,1:], axis=-1)\n",
    "        y_pred = tf.argmax(y_pred[:,:,1:], axis=-1)\n",
    "                \n",
    "        # miou_op.reset_states()\n",
    "        miou_op.update_state(y_true, y_pred)\n",
    "        iou_macro = miou_op.result().numpy()\n",
    "        iou_macro_scores[idx] = iou_macro\n",
    "        \n",
    "        if idx == (n_samples-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Average inference time: {:.2f}s\".format(np.mean(inf_times)))\n",
    "            \n",
    "    return iou_macro_scores, miou_op\n",
    "\n",
    "\n",
    "def mean_over_valid(x):\n",
    "    return np.mean(x[x != -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: Make mean IoU a metric, so you can do\n",
    "\n",
    "```python\n",
    "loss, accuracy, miou = model.evaluate(dataset)\n",
    "```\n",
    "\n",
    "Might need to write a custom training loop to reset metric states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iou_macro_scores, miou_op = evaluate_iou(model=model, dataset=test, n_samples=TEST_LENGTH)\n",
    "# iou_macro_scores, miou_op = evaluate_iou(model=model, dataset=eval, n_samples=TEST_LENGTH)\n",
    "iou_mean_macro = np.mean(iou_macro_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_int = np.sum(miou_op.get_weights()[0], axis=0)+np.sum(miou_op.get_weights()[0], axis=1)\n",
    "inters = np.diag(miou_op.get_weights()[0])\n",
    "ious = inters / (union_int-inters+1)\n",
    "\n",
    "print(\"Mean IoU: {:.4f} \\n\".format(iou_mean_macro))\n",
    "for i in range(ious.shape[0]) :\n",
    "    print(\"IoU for {}: {:.2f}\".format(trainid2label[i+1].name, np.round(ious[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_trainId(\n",
    "    trainId_label_map=trainid2label,\n",
    "    catId_label_map=catId2label, \n",
    "    n_classes=n_classes, \n",
    "    iou_class=ious,\n",
    "    model=model, \n",
    "    iou_mean=iou_mean_macro,\n",
    "    current_dir=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(confusion, metric, label_classes, model):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.title(\"{} Confusion Matrix, with Mean IoU = {:.3f}\".format(model.name, metric), fontsize=22)\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-90, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_confusion_matrix(\n",
    "    confusion = miou_op.get_weights()[0] / np.sum(miou_op.get_weights()[0], axis=0), \n",
    "    metric = iou_mean_macro, \n",
    "    label_classes = [trainid2label[i].name for i in range(1, n_classes)],\n",
    "    model = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
