{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "from utils.plot_utils import plot_iou_trainId, plot_iou_catId\n",
    "from utils.data_utils import get_labels, parse_record, get_dataset_from_tfrecord\n",
    "from models.hrnet_keras import HRNet\n",
    "from models.u2net import U2NET\n",
    "from models.erfnet import ERFNet\n",
    "from data_loader import DataLoader\n",
    "\n",
    "K.clear_session()\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "\n",
    "def enable_amp():\n",
    "    mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "    \n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "print(physical_devices,\"\\n\")\n",
    "# enable_amp() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('ggplot')\n",
    "plt.rc('xtick',labelsize=16)\n",
    "plt.rc('ytick',labelsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
    "# sudo apt-get install apt-transport-https ca-certificates gnupg\n",
    "# curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add -\n",
    "# sudo apt-get update && sudo apt-get install google-cloud-sdk\n",
    "# gcloud init --console-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_train.record /home/ubuntu/cityscapes_cv/records/\n",
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_val.record /home/ubuntu/cityscapes_cv/records/\n",
    "# gsutil cp gs://cl_datasets_01/cityscapes/records/trainIds_train_extra.record /home/ubuntu/cityscapes_cv/records/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine = True\n",
    "\n",
    "if fine:\n",
    "    train_tfrecord_dir = \"records/trainIds_train.record\"\n",
    "    test_tfrecord_dir = \"records/trainIds_val.record\"\n",
    "else:\n",
    "    train_tfrecord_dir = \"records/trainIds_train_extra.record\"\n",
    "\n",
    "img_height = 64 # 512\n",
    "img_width = 128 # 1024\n",
    "n_classes = 20\n",
    "\n",
    "labels = get_labels()\n",
    "trainid2label = { label.trainId : label for label in labels }\n",
    "catId2label = { label.categoryId : label for label in labels }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine:\n",
    "    TRAIN_LENGTH = 2975\n",
    "    TEST_LENGTH = 500\n",
    "else:\n",
    "    TRAIN_LENGTH = 18000\n",
    "    TEST_LENGTH = 1998\n",
    "\n",
    "BATCH_SIZE = 6\n",
    "BUFFER_SIZE = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DataLoader(img_height=img_height, img_width=img_width, n_classes=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if fine:\n",
    "    train_ds = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "    test_ds = get_dataset_from_tfrecord(test_tfrecord_dir)\n",
    "else:\n",
    "    all_ds = get_dataset_from_tfrecord(train_tfrecord_dir)\n",
    "    train_ds = all_ds.skip(TEST_LENGTH)\n",
    "    test_ds = all_ds.take(TEST_LENGTH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: random crop the images and masks, flip them\n",
    "train = train_ds.map(pipeline.load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = test_ds.map(pipeline.load_image_test)\n",
    "eval = test_ds.map(pipeline.load_image_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_rgb(mask):\n",
    "    h = mask.shape[0]\n",
    "    w = mask.shape[1]\n",
    "    mask_rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for val, key in trainid2label.items():\n",
    "        indices = mask == val\n",
    "        mask_rgb[indices.squeeze()] = key.color \n",
    "    return mask_rgb\n",
    "\n",
    "\n",
    "def display(display_list, title=True):\n",
    "    plt.figure(figsize=(15, 5)) # dpi=200\n",
    "    if title:\n",
    "        title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        if title:\n",
    "            plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in train.take(21): # 16\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "sample_mask = tf.argmax(sample_mask, axis=-1)\n",
    "sample_mask = sample_mask[..., tf.newaxis]\n",
    "sample_mask = label_to_rgb(sample_mask.numpy())\n",
    "display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if fine:\n",
    "    MODEL_PATH = \"weights/HRNet_W18.h5\"\n",
    "else:\n",
    "    MODEL_PATH = \"weights/HRNet_W18_coarse.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.squeeze(pred_mask)\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    pred_mask = label_to_rgb(pred_mask.numpy())\n",
    "    return pred_mask\n",
    "\n",
    "\n",
    "def show_predictions():\n",
    "    pred_mask = model.predict(sample_image[tf.newaxis, ...])\n",
    "    if \"u2net\" in model.name:\n",
    "        pred_mask = pred_mask[0]\n",
    "    display([sample_image, sample_mask, create_mask(pred_mask)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "test_dataset = strategy.experimental_distribute_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger_file = \"cityscapes_fine_hrnet.csv\"\n",
    "\n",
    "c_weights = [0,      0.8373, 0.918,  0.866,  1.0345, \n",
    "             1.0166, 0.9969, 0.9754, 1.0489, 0.8786,\n",
    "             1.0023, 0.9539, 0.9843, 1.1116, 0.9037,\n",
    "             1.0865, 1.0955, 1.0865, 1.1529, 1.0507]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    learning_rate_fn = PolynomialDecay(\n",
    "        initial_learning_rate = 0.01,\n",
    "        decay_steps = 12000,\n",
    "        end_learning_rate=1e-5,\n",
    "        power=0.9\n",
    "    )\n",
    "\n",
    "    # model = U2NET(input_height=img_height, input_width=img_width, n_classes=n_classes)\n",
    "    # model = HRNet(input_height=img_height, input_width=img_width, n_classes=20, W=40)\n",
    "    model = HRNet(input_height=img_height, input_width=img_width, n_classes=20, W=18)\n",
    "\n",
    "    optimizer = SGD(learning_rate=learning_rate_fn, momentum=0.9, decay=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_coef(y_true, y_pred):\n",
    "    y_true = tf.nn.softmax(y_true, axis=-1)\n",
    "    smooth = 1\n",
    "    iou_total = 0\n",
    "    for i in range(1, n_classes):\n",
    "        tp = tf.math.reduce_sum(y_pred[:,:,:,i] * y_true[:,:,:,i], axis=(1,2))\n",
    "        fn = tf.math.reduce_sum(y_true[:,:,:,i] * (1 - y_pred[:,:,:,i]), axis=(1,2)) \n",
    "        fp = tf.math.reduce_sum(y_pred[:,:,:,i] * (1 - y_true[:,:,:,i]), axis=(1,2)) \n",
    "        iou = tf.math.reduce_mean(tf.math.divide_no_nan(tp+smooth, tp+fn+fp+smooth), axis=0)\n",
    "        iou_total += iou\n",
    "\n",
    "    iou_macro = iou_total / (n_classes - 1)\n",
    "    return iou_macro\n",
    "\n",
    "\n",
    "loss_object = CategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    #class_weights = tf.constant([c_weights])\n",
    "    #weights_processed = tf.reduce_sum(class_weights * y_true, axis=-1)\n",
    "    per_example_loss = loss_object(y_true, y_pred)\n",
    "    #per_example_loss = weights_processed * per_example_loss\n",
    "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "def train_step(inputs):\n",
    "    x, y = inputs\n",
    "    with tf.GradientTape() as tape :\n",
    "        output = model(x, training=True)\n",
    "        loss = compute_loss(y, output)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(dist_inputs):\n",
    "    per_replica_losses = strategy.run(train_step, args=(dist_inputs,))\n",
    "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None)\n",
    "\n",
    "\n",
    "def test_step(inputs):\n",
    "    x, y = inputs\n",
    "    output = model(x, training=False)\n",
    "    loss = compute_loss(y, output)\n",
    "    miou = iou_coef(y, output)\n",
    "    return loss, miou\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def distributed_test_step(dist_inputs):\n",
    "    pe_loss, pe_miou = strategy.run(test_step, args=(dist_inputs,))\n",
    "    loss = strategy.reduce(tf.distribute.ReduceOp.SUM, pe_loss, axis=None)\n",
    "    miou = strategy.reduce(tf.distribute.ReduceOp.MEAN, pe_miou, axis=None)\n",
    "    return loss, miou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    \n",
    "    logfile = Path(logger_file)\n",
    "\n",
    "    if not logfile.exists() :\n",
    "        tmpf = logfile.open(\"w+\")\n",
    "        top_text = \"epoch, loss, val_loss, miou\\n\"\n",
    "        tmpf.write(top_text)\n",
    "        tmpf.close()\n",
    "    \n",
    "    history = {\"loss\": [], \"val_loss\": [], \"val_iou_coef\": []}\n",
    "    \n",
    "    with strategy.scope():\n",
    "    \n",
    "        start_time = time()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_start_time = time()\n",
    "            \n",
    "            train_losses = []\n",
    "            valid_losses = []\n",
    "            valid_mious = []\n",
    "\n",
    "            for step, train_batch in enumerate(train_dataset):\n",
    "                loss = distributed_train_step(train_batch)\n",
    "                train_losses.append(loss.numpy())\n",
    "                print(\"\\r Batch {} -- loss: {:.4f}\".format(step, loss.numpy()), end='')\n",
    "\n",
    "            for val_batch in test_dataset:\n",
    "                loss, miou = distributed_test_step(val_batch)\n",
    "                valid_losses.append(loss.numpy())\n",
    "                valid_mious.append(miou.numpy())\n",
    "\n",
    "            train_loss = np.mean(train_losses)\n",
    "            valid_loss = np.mean(valid_losses)\n",
    "            valid_miou = np.mean(valid_mious)\n",
    "\n",
    "            history['loss'].append(train_loss)\n",
    "            history['val_loss'].append(valid_loss)\n",
    "            history['val_iou_coef'].append(valid_miou)\n",
    "            \n",
    "            tmpf = logfile.open(\"a+\")\n",
    "            tmpf.write(\",\".join([str(epoch), str(train_loss), str(valid_loss), str(valid_miou)]) + \"\\n\")\n",
    "            tmpf.close()\n",
    "            \n",
    "            t_epoch = time() - epoch_start_time\n",
    "            template = '\\n Epoch {} -- Time: {:.2f}s, Loss: {:.4f}, Val Loss: {:.4f}, Val mIoU: {:.4f}'\n",
    "            print(template.format(epoch+1, t_epoch, train_loss, valid_loss, valid_miou))\n",
    "            \n",
    "            model.save(MODEL_PATH)\n",
    "            show_predictions()\n",
    "            \n",
    "    end_time = time()\n",
    "    t_minutes = (end_time - start_time) // 60\n",
    "    print(\"Training finished in {:.2f} minutes\".format(t_minutes))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model, history = train(epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, model):\n",
    "        \n",
    "    plt.figure(figsize=(15,7))\n",
    "    plt.subplot(1,2,1)  \n",
    "    if \"u2net\" in model.name:\n",
    "        plt.plot(history['d0_loss'], 'r', label='Training loss')\n",
    "        plt.plot(history['val_d0_loss'], 'b', label='Validation loss')\n",
    "    else: \n",
    "        plt.plot(history['loss'], 'r', label='Training loss')\n",
    "        plt.plot(history['val_loss'], 'b', label='Validation loss')\n",
    "    plt.title(\"Loss: \"+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    if \"u2net\" in model.name:\n",
    "        plt.plot(history['d0_iou_coef'], 'r', label='IoU coefficient')\n",
    "        plt.plot(history['val_d0_iou_coef'], 'b', label='Validation IoU coefficient')\n",
    "    else:\n",
    "        plt.plot(history['iou_coef'], 'r', label='IoU coefficient')\n",
    "        plt.plot(history['val_iou_coef'], 'b', label='Validation IoU coefficient')\n",
    "    plt.title('IoU Coefficient: '+model.name, fontsize=16)\n",
    "    plt.xlabel('Epoch', fontsize=16)\n",
    "    plt.legend(prop={'size': 14})\n",
    "    if fine:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves.png\")\n",
    "    else:\n",
    "        plt.savefig(\"plots/\"+model.name+\"_learning_curves_coarse.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_iou(model, dataset, n_samples):\n",
    "    \n",
    "    iou_scores = np.zeros((n_samples,))\n",
    "    inf_times = np.zeros((n_samples, ))\n",
    "    miou_op =  tf.keras.metrics.MeanIoU(num_classes=n_classes-1)\n",
    "    \n",
    "    for idx, (image, mask) in enumerate(dataset):\n",
    "        print(\"\\r Predicting {} \\ {} \".format(idx+1, n_samples), end='')\n",
    "        \n",
    "        X = np.expand_dims(image.numpy(), axis=0)\n",
    "        y_true = np.expand_dims(mask.numpy(), axis=0)\n",
    "        \n",
    "        t_start = time()\n",
    "        y_pred = model.predict(X)\n",
    "        t_end = time()\n",
    "        t_inf = t_end-t_start\n",
    "        \n",
    "        inf_times[idx] = t_inf\n",
    "        \n",
    "        if \"U2Net\" in model.name:\n",
    "            y_pred = y_pred[0]\n",
    "            \n",
    "        # y_pred = tf.image.resize(y_pred, (1024, 2048))\n",
    "        threshold = tf.math.reduce_max(y_pred, axis=-1, keepdims=True)\n",
    "        y_pred = tf.logical_and(y_pred >= threshold, tf.abs(y_pred) > 1e-12)\n",
    "        \n",
    "        y_pred = tf.cast(tf.squeeze(y_pred, axis=0), tf.int32)\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=0), tf.int32)\n",
    "        \n",
    "        y_true = tf.argmax(y_true[:,:,1:], axis=-1)\n",
    "        y_pred = tf.argmax(y_pred[:,:,1:], axis=-1)\n",
    "                \n",
    "        # miou_op.reset_states()\n",
    "        miou_op.update_state(y_true, y_pred)\n",
    "        iou = miou_op.result().numpy()\n",
    "        iou_scores[idx] = iou\n",
    "        \n",
    "        if idx == (n_samples-1):\n",
    "            break\n",
    "    \n",
    "    print(\"Average inference time: {:.2f}s\".format(np.mean(inf_times)))\n",
    "            \n",
    "    return iou_scores, miou_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iou_macro, miou_op = evaluate_iou(model=model, dataset=eval, n_samples=TEST_LENGTH)\n",
    "iou_mean = np.mean(iou_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_int = np.sum(miou_op.get_weights()[0], axis=0)+np.sum(miou_op.get_weights()[0], axis=1)\n",
    "inters = np.diag(miou_op.get_weights()[0])\n",
    "ious = inters / (union_int-inters+1)\n",
    "\n",
    "print(\"Mean IoU: {:.4f} \\n\".format(iou_mean))\n",
    "for i in range(ious.shape[0]) :\n",
    "    print(\"IoU for {}: {:.2f}\".format(trainid2label[i+1].name, np.round(ious[i], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_catId(catId_label_map, n_classes, iou_class, model, iou_mean):\n",
    "\n",
    "    categories = [catId_label_map[i].category for i in range(1, n_classes)]\n",
    "    cat_colors = {\n",
    "        'void': colors.to_hex(list(np.array(catId_label_map[0].color)/255)),\n",
    "        'flat': colors.to_hex(list(np.array(catId_label_map[1].color)/255)),\n",
    "        'construction': colors.to_hex(list(np.array(catId_label_map[2].color)/255)),\n",
    "        'object': colors.to_hex(list(np.array(catId_label_map[3].color)/255)),\n",
    "        'nature': colors.to_hex(list(np.array(catId_label_map[4].color)/255)),\n",
    "        'sky': colors.to_hex(list(np.array(catId_label_map[5].color)/255)),\n",
    "        'human': colors.to_hex(list(np.array(catId_label_map[6].color)/255)),\n",
    "        'vehicle': colors.to_hex(list(np.array(catId_label_map[7].color)/255))\n",
    "    }\n",
    "    _colors = [cat_colors[category] for category in categories]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14,10))\n",
    "    hbars = ax.barh(categories, iou_class, color=_colors)\n",
    "    \n",
    "    ax.set_xlabel(\"IoU Coefficient: \", fontsize=18)\n",
    "    ax.set_ylabel(\"Category Name\", fontsize=18)\n",
    "    ax.set_title(\"Category Scores for {} - Mean IoU: {:.3f}\".format(model.name, iou_mean), fontsize=22)\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # ax.bar_label(hbars, fmt=\"%.2f\", padding=3, fontsize=16)\n",
    "    \n",
    "    plt.savefig(\"plots/\"+model.name+\"_category_iou_scores.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_iou_trainId(trainId_label_map, catId_label_map, n_classes, iou_class, model, iou_mean):\n",
    "\n",
    "    categories = [trainId_label_map[i].category for i in range(1, n_classes)]\n",
    "    \n",
    "    cat_colors = {\n",
    "        'void': colors.to_hex(list(np.array(catId_label_map[0].color)/255)),\n",
    "        'flat': colors.to_hex(list(np.array(catId_label_map[1].color)/255)),\n",
    "        'construction': colors.to_hex(list(np.array(catId_label_map[2].color)/255)),\n",
    "        'object': colors.to_hex(list(np.array(catId_label_map[3].color)/255)),\n",
    "        'nature': colors.to_hex(list(np.array(catId_label_map[4].color)/255)),\n",
    "        'sky': colors.to_hex(list(np.array(catId_label_map[5].color)/255)),\n",
    "        'human': colors.to_hex(list(np.array(catId_label_map[6].color)/255)),\n",
    "        'vehicle': colors.to_hex(list(np.array(catId_label_map[7].color)/255))\n",
    "    }\n",
    "    _colors = [cat_colors[category] for category in categories]\n",
    "\n",
    "    names = [trainId_label_map[i].name for i in range(1, n_classes)]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14,10))\n",
    "    hbars = ax.barh(names, iou_class, color=_colors)\n",
    "    \n",
    "    ax.set_xlabel(\"IoU Coefficient: \", fontsize=18)\n",
    "    ax.set_ylabel(\"Class Name\", fontsize=18)\n",
    "    ax.set_title(\"Class Scores for {} - Mean IoU: {:.3f}\".format(model.name, iou_mean), fontsize=22)\n",
    "    ax.set_xlim([0, 1])\n",
    "    \n",
    "    # ax.bar_label(hbars, fmt=\"%.2f\", padding=3, fontsize=16)\n",
    "    \n",
    "    plt.savefig(\"plots/\"+model.name+\"_class_iou_scores.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_trainId(\n",
    "    trainId_label_map=trainid2label,\n",
    "    catId_label_map=catid2label, \n",
    "    n_classes=n_classes, \n",
    "    iou_class=ious,\n",
    "    model=model, \n",
    "    iou_mean=iou_mean,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(confusion, metric, label_classes, model):\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.title(\"{} Confusion Matrix, with Mean IoU = {:.3f}\".format(model.name, metric), fontsize=22)\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-90, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    ax.grid(False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_confusion_matrix(\n",
    "    confusion = miou_op.get_weights()[0] / np.sum(miou_op.get_weights()[0], axis=0), \n",
    "    metric = iou_mean_macro, \n",
    "    label_classes = [trainid2label[i].name for i in range(1, n_classes)],\n",
    "    model = model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
